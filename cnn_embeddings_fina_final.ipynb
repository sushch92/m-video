{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 960 (0000:01:00.0)\n",
      "/home/svetlana/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.feature_extraction.text import _document_frequency\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras import backend as K\n",
    "#from keras.regularizers import l2, activity_l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Nadam\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cluster import KMeans\n",
    "import word2vecReader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier as RandForest\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists('business.json'):\n",
    "    pass\n",
    "else:\n",
    "    !wget https://s3.amazonaws.com/cloudfinalprojectsubmission/business.json\n",
    "\n",
    "if os.path.exists('review.json'):\n",
    "    pass\n",
    "else:\n",
    "    !wget https://s3.amazonaws.com/cloudfinalprojectsubmission/review.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_size = 100000\n",
    "\n",
    "\n",
    "usa_business_idx = []\n",
    "business_reader = pd.read_table('business.json', header=None, chunksize=chunk_size)\n",
    "categories = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    for i, chunks in enumerate(business_reader):\n",
    "\n",
    "        start_index = chunk_size * i\n",
    "\n",
    "        for index in range(start_index, start_index + len(chunks)):\n",
    "\n",
    "            data_json = pd.read_json(chunks[0][index], typ='series')\n",
    "            if data_json['city'] in ['Pittsburgh', 'Charlotte', 'Urbana-Champaign',\n",
    "                                     'Phoenix', 'Las Vegas', 'Madison']:\n",
    "                usa_business_idx.append(data_json['business_id'])\n",
    "                categories = categories.append(data_json[['business_id', 'categories']], ignore_index=True)\n",
    "except:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = categories.drop_duplicates('business_id')\n",
    "categories = categories.set_index('business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11487, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_reader = pd.read_table('review.json', header=None, chunksize=chunk_size)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, chunks in enumerate(review_reader):\n",
    "\n",
    "        start_index = chunk_size * i\n",
    "        for index in range(start_index, start_index + len(chunks)):\n",
    "                data_json = pd.read_json(chunks[0][index], typ='series')\n",
    "                bus_id = data_json['business_id']\n",
    "                if (bus_id in usa_business_idx) and (u'Restaurants' in categories.loc[bus_id].categories):\n",
    "                    df = df.append(data_json[['text', 'stars']], ignore_index=True)\n",
    "                    if data_json['stars'] < 4.:\n",
    "                        df = df.append(data_json[['text', 'stars']], ignore_index=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['stars'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/media/svetlana/DATA/dir/this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from word2vecReader import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = 'word2vec_twitter_model/word2vec_twitter_model.bin'\n",
    "word2vec_model = Word2Vec.load_word2vec_format(fname, binary=True)  # C binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from word2vecReader import Word2Vec\n",
    "import gensim\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "fname = 'word2vec_twitter_model/word2vec_twitter_model.bin'\n",
    "word2vec_model = Word2Vec.load_word2vec_format(fname, binary=True)  # C binary format\n",
    "max_words_count = max((len(tokenizer.tokenize(sent)) for sent in X_train.values))\n",
    "word2vec_dim = 400 # ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "group_lb = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "words_set = set(reduce(lambda x, y: x + y, map(lambda x: tokenizer.tokenize(x), X_train.values.tolist())))\n",
    "\n",
    "index_dict = {item: (number + 1) for number, item in enumerate(words_set)}\n",
    "n_symbols = len(index_dict) + 1 # adding 1 to account for 0th index (for masking)\n",
    "embedding_weights = np.zeros((len(index_dict) + 1, word2vec_dim))\n",
    "for word, index in index_dict.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_weights[index,:] = word2vec_model[word] / np.linalg.norm(word2vec_model[word])\n",
    "    else:\n",
    "        embedding_weights[index, :] = np.zeros(word2vec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words_count = 30\n",
    "def for_embedding(text, word_index):\n",
    "    words = tokenizer.tokenize(text)\n",
    "    words_idxes = np.zeros(max_words_count)\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        if (word in word_index) and (i < max_words_count - 1):\n",
    "            words_idxes[i] = word_index[word]\n",
    "            i += 1\n",
    "    return words_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.apply(lambda x: for_embedding(x, index_dict)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = group_lb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = np.array(X_test.apply(lambda x: for_embedding(x, index_dict)).tolist())\n",
    "y_test = group_lb.fit_transform(y_test)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, MaxPool1D, Flatten\n",
    "from keras.layers.convolutional import Conv1D, Convolution1D, Convolution2D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"OMP_NUM_THREADS=8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def my_init(shape, dtype=None):\n",
    "    return K.random_normal(shape, stddev=0.05, dtype=dtype)     \n",
    "\n",
    "def conv_model(max_words_count, words_set, word2vec_dim, embedding_weights):\n",
    "    \n",
    "    word2vec_shape = 400\n",
    "    inputs = Input(shape=(max_words_count,), dtype=\"float32\", name=\"inputs_a\")\n",
    "    \n",
    "    embedding = Embedding(len(words_set) + 1, word2vec_dim, input_length=max_words_count, \n",
    "              weights=[embedding_weights], trainable=False, mask_zero=False)(inputs)\n",
    "    embedding = Dropout(0.5)(embedding)\n",
    "    l_conv1 = Conv1D(filters=1024, kernel_size=3)(embedding)\n",
    "    l_pool1 = MaxPool1D(pool_size=3)(l_conv1)\n",
    "    \n",
    "    \n",
    "    #l_conv2 = Conv1D(filters=256, kernel_size=3)(l_pool1)\n",
    "    #l_pool2 = MaxPool1D(pool_size=3)(l_conv2)\n",
    "    \n",
    "    #l_conv3 = Conv1D(filters=256, kernel_size=3)(l_pool2)\n",
    "    #l_pool3 = MaxPool1D(pool_size=3)(l_conv3)\n",
    "    \n",
    "    flat = Flatten()(l_pool1)\n",
    "    \n",
    "    fc_layer1 = Dense(3000, activation='relu')(flat)\n",
    "    fc_layer1 = BatchNormalization()(fc_layer1)\n",
    "    drop1_out = Dropout(0.5)(fc_layer1)\n",
    "    \n",
    "    fc_layer2 = Dense(1000, activation='relu')(drop1_out)\n",
    "    fc_layer2 = BatchNormalization()(fc_layer2)\n",
    "    drop1_out = Dropout(0.5)(fc_layer2)\n",
    "    \n",
    "    fc_layer3 = Dense(500, activation='relu')(drop1_out)\n",
    "    fc_layer3 = BatchNormalization()(fc_layer3)\n",
    "    drop1_out = Dropout(0.5)(fc_layer3)\n",
    "    \n",
    "    \n",
    "    #fc_layer2 = Dense(256, activation='relu')(drop1_out)\n",
    "        \n",
    "    outputs = Dense(5, activation='softmax', name='outputs')(drop1_out)\n",
    "    my_loss = {'outputs': 'categorical_crossentropy'}\n",
    "\n",
    "    inputs_all = [\n",
    "                inputs\n",
    "            ]\n",
    "\n",
    "    outputs_all = [\n",
    "                outputs\n",
    "            ]\n",
    "\n",
    "    model = Model(inputs=inputs_all, outputs=outputs_all, )\n",
    "    nadam = Nadam(lr=1e-2)\n",
    "    #adadelta = Adadelta()\n",
    "    model.compile(\n",
    "                optimizer=nadam,\n",
    "                loss=my_loss,\n",
    "                loss_weights={'outputs': 1.},\n",
    "                metrics = ['accuracy']\n",
    "              )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs_a (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_23 (Embedding)     (None, 30, 400)           15080800  \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 28, 1024)          1229824   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 9, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 3000)              27651000  \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 3000)              12000     \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1000)              3001000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 47,483,629\n",
      "Trainable params: 32,393,829\n",
      "Non-trainable params: 15,089,800\n",
      "_________________________________________________________________\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 2.3670 - acc: 0.2544 - val_loss: 1.5190 - val_acc: 0.3172\n",
      "('epoch: ', 0)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 1.9928 - acc: 0.2981 - val_loss: 1.4091 - val_acc: 0.3798\n",
      "('epoch: ', 1)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 28s - loss: 1.7850 - acc: 0.3429 - val_loss: 1.3475 - val_acc: 0.4112\n",
      "('epoch: ', 2)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 1.6569 - acc: 0.3704 - val_loss: 1.3228 - val_acc: 0.4156\n",
      "('epoch: ', 3)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 27s - loss: 1.5544 - acc: 0.3983 - val_loss: 1.2888 - val_acc: 0.4471\n",
      "('epoch: ', 4)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 27s - loss: 1.4442 - acc: 0.4315 - val_loss: 1.2621 - val_acc: 0.4618\n",
      "('epoch: ', 5)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 1.3805 - acc: 0.4486 - val_loss: 1.2213 - val_acc: 0.4867\n",
      "('epoch: ', 6)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 1.2797 - acc: 0.4840 - val_loss: 1.2085 - val_acc: 0.4815\n",
      "('epoch: ', 7)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 1.2098 - acc: 0.5130 - val_loss: 1.2031 - val_acc: 0.4854\n",
      "('epoch: ', 8)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 1.1194 - acc: 0.5442 - val_loss: 1.1902 - val_acc: 0.4793\n",
      "('epoch: ', 9)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 27s - loss: 1.0640 - acc: 0.5742 - val_loss: 1.1342 - val_acc: 0.5291\n",
      "('epoch: ', 10)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 27s - loss: 0.9924 - acc: 0.6048 - val_loss: 1.1039 - val_acc: 0.5518\n",
      "('epoch: ', 11)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 27s - loss: 0.9307 - acc: 0.6347 - val_loss: 1.1086 - val_acc: 0.5410\n",
      "('epoch: ', 12)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.8741 - acc: 0.6537 - val_loss: 1.1377 - val_acc: 0.5447\n",
      "('epoch: ', 13)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.8072 - acc: 0.6794 - val_loss: 1.1021 - val_acc: 0.5415\n",
      "('epoch: ', 14)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.7591 - acc: 0.7036 - val_loss: 1.0245 - val_acc: 0.5876\n",
      "('epoch: ', 15)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.7123 - acc: 0.7222 - val_loss: 1.0586 - val_acc: 0.5896\n",
      "('epoch: ', 16)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.6679 - acc: 0.7451 - val_loss: 1.0202 - val_acc: 0.6065\n",
      "('epoch: ', 17)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.6243 - acc: 0.7618 - val_loss: 1.1114 - val_acc: 0.5683\n",
      "('epoch: ', 18)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.5911 - acc: 0.7721 - val_loss: 1.0404 - val_acc: 0.6030\n",
      "('epoch: ', 19)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.5522 - acc: 0.7897 - val_loss: 1.1662 - val_acc: 0.5679\n",
      "('epoch: ', 20)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.5103 - acc: 0.8083 - val_loss: 0.9867 - val_acc: 0.6292\n",
      "('epoch: ', 21)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.4790 - acc: 0.8209 - val_loss: 1.0478 - val_acc: 0.6256\n",
      "('epoch: ', 22)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.4770 - acc: 0.8230 - val_loss: 1.0545 - val_acc: 0.6284\n",
      "('epoch: ', 23)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.4196 - acc: 0.8419 - val_loss: 1.1156 - val_acc: 0.6267\n",
      "('epoch: ', 24)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.3885 - acc: 0.8552 - val_loss: 1.1192 - val_acc: 0.6314\n",
      "('epoch: ', 25)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.3632 - acc: 0.8637 - val_loss: 1.0905 - val_acc: 0.6376\n",
      "('epoch: ', 26)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 26s - loss: 0.3376 - acc: 0.8758 - val_loss: 1.1463 - val_acc: 0.6329\n",
      "('epoch: ', 27)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.3290 - acc: 0.8824 - val_loss: 1.3184 - val_acc: 0.6247\n",
      "('epoch: ', 28)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.3042 - acc: 0.8896 - val_loss: 1.2596 - val_acc: 0.6297\n",
      "('epoch: ', 29)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.2839 - acc: 0.8963 - val_loss: 1.2586 - val_acc: 0.6292\n",
      "('epoch: ', 30)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.2721 - acc: 0.9014 - val_loss: 1.4160 - val_acc: 0.6246\n",
      "('epoch: ', 31)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.2518 - acc: 0.9043 - val_loss: 1.3933 - val_acc: 0.6239\n",
      "('epoch: ', 32)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.2184 - acc: 0.9207 - val_loss: 1.3354 - val_acc: 0.6367\n",
      "('epoch: ', 33)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 25s - loss: 0.2241 - acc: 0.9186 - val_loss: 1.3694 - val_acc: 0.6313\n",
      "('epoch: ', 34)\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 35\n",
    "\n",
    "model = conv_model(max_words_count=max_words_count,\n",
    "                        words_set=words_set,\n",
    "                        word2vec_dim=word2vec_dim,\n",
    "                        embedding_weights=embedding_weights)\n",
    "model.optimizer.lr.set_value(0.0001)\n",
    "\n",
    "for epoch_num in range(nb_epoch):\n",
    "    fit = model.fit(X_train, y_train, batch_size=32, epochs=1, verbose=1, validation_data=(X_test, y_test),)\n",
    "    res = model.predict(X_test)\n",
    "    res = res\n",
    "    print(\"epoch: \", epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
